//! Networked POS Node with Auto-Discovery
//!
//! Features:
//! - L0 (Discovery): mDNS-based peer discovery on LAN
//! - L2 (Transport): QUIC-based high-speed data transport
//! - Automatic handover from discovery to data plane

use mimalloc::MiMalloc;

#[global_allocator]
static GLOBAL: MiMalloc = MiMalloc;

use clap::Parser;
use futures::stream::StreamExt;
use libp2p::{
    mdns, noise,
    swarm::{NetworkBehaviour, SwarmEvent},
    tcp, yamux, Multiaddr, PeerId, SwarmBuilder,
};
use quinn::{ClientConfig, Endpoint, ServerConfig};

use std::net::{IpAddr, Ipv4Addr, SocketAddr};
use std::sync::atomic::{AtomicU64, AtomicUsize, Ordering};
use std::sync::Arc;
use std::time::Duration;
use tokio::sync::mpsc;
use tracing::{info, warn};

use pos::{
    messages::{deserialize_message, serialize_message},
    ArenaMempool, GeometricLedger, Sequencer, SequencerConfig, Transaction, TransactionPayload,
    WireMessage, PROTOCOL_VERSION,
};

#[derive(Parser, Debug)]
#[command(author, version, about = "Networked POS Node with Auto-Discovery")]
struct Args {
    /// QUIC port for high-speed data transport (L2)
    #[arg(short, long, default_value_t = 9000)]
    port: u16,

    /// TCP port for mDNS discovery (L0) - 0 means OS picks random port
    #[arg(short, long, default_value_t = 0)]
    discovery_port: u16,

    /// Node ID (for debugging)
    #[arg(short, long)]
    node_id: Option<String>,

    /// Enable producer mode (generate test transactions)
    #[arg(long, default_value_t = false)]
    producer: bool,

    /// Transactions per second to generate (producer mode)
    #[arg(long, default_value_t = 2_000_000)]
    tps: u64,

    /// Batch size for sequencer (optimal: 45k for L3 cache)
    #[arg(long, default_value_t = 45_000)]
    batch_size: usize,

    /// Generate valid geometric positions (100% acceptance rate)
    /// Without this flag, random positions are generated (~20% acceptance)
    #[arg(long, default_value_t = false)]
    smart_gen: bool,

    /// Maximum mempool size (default: 1M transactions)
    #[arg(long, default_value_t = 1_000_000)]
    mempool_size: usize,
}

#[derive(Debug, Clone)]
struct DiscoveredPeer {
    peer_id: PeerId,
    multiaddr: Multiaddr,
    quic_port: u16,
}

// Custom behaviour: just mDNS for local discovery
#[derive(NetworkBehaviour)]
struct DiscoveryBehaviour {
    mdns: mdns::tokio::Behaviour,
}

type ShardSender = std::sync::mpsc::SyncSender<ShardWork>;

#[tokio::main]
async fn main() -> Result<(), Box<dyn std::error::Error>> {
    // Initialize tracing
    tracing_subscriber::fmt()
        .with_env_filter(
            tracing_subscriber::EnvFilter::try_from_default_env()
                .unwrap_or_else(|_| tracing_subscriber::EnvFilter::new("info")),
        )
        .with_target(false)
        .init();

    let args = Args::parse();
    let node_name = args
        .node_id
        .clone()
        .unwrap_or_else(|| format!("node-{}", args.port));

    println!("‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó");
    println!("‚ïë           POS Protocol - Networked Node                       ‚ïë");
    println!("‚ï†‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï£");
    println!("‚ïë  L0 (Discovery): mDNS-based peer discovery                    ‚ïë");
    println!("‚ïë  L2 (Transport): QUIC high-speed data plane                   ‚ïë");
    println!("‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù");
    println!();
    println!("üöÄ Starting {} on QUIC port {}", node_name, args.port);
    println!();

    // Initialize Geometric Ledger
    let db_path = format!("./data/geometric_ledger_{}", args.port);
    let ledger =
        Arc::new(GeometricLedger::new(&db_path).expect("Failed to initialize GeometricLedger"));

    // Initialize sequencer config template
    let sequencer_config = SequencerConfig {
        sequencer_id: generate_node_id(&node_name),
        batch_size: args.batch_size,
        ..Default::default()
    };

    // Pre-mint tokens for test accounts (always)
    // We mint for 1M accounts to match the client's load test range
    // CRITICAL: This must happen BEFORE starting the network to avoid race conditions
    info!("üí∞ Pre-minting balances for 1,000,000 accounts (batched)...");

    // Batch updates to avoid 1M separate Rayon calls and disk flushes
    const BATCH_SIZE: usize = 10_000;
    let mut batch = Vec::with_capacity(BATCH_SIZE);

    for i in 0..1_000_000u64 {
        let sender_seed = i.to_le_bytes();
        let secret_hash = blake3::hash(&sender_seed);
        let secret = *secret_hash.as_bytes();
        let sender_hash = blake3::hash(&secret);
        let sender_addr = *sender_hash.as_bytes();

        // Create account with balance
        let acc = pos::Account {
            pubkey: sender_addr,
            balance: 1_000_000_000,
            ..Default::default()
        };

        batch.push((sender_addr, acc));

        if batch.len() >= BATCH_SIZE {
            ledger
                .update_batch(&batch)
                .expect("Failed to pre-mint batch");
            batch.clear();
        }
    }
    // Flush remaining
    if !batch.is_empty() {
        ledger
            .update_batch(&batch)
            .expect("Failed to pre-mint final batch");
    }
    info!("‚úÖ Pre-minting complete!");

    // Threading Model
    let total_cores = num_cpus::get();
    let num_pairs = (total_cores / 2).max(1).min(pos::ARENA_MAX_WORKERS);
    info!(
        "‚öôÔ∏è Threading Model: {} Producer/Consumer pairs (using {} cores)",
        num_pairs,
        num_pairs * 2
    );

    // Initialize Arena Mempool
    // Reduced to 256MB to prevent OOM on Mac during heavy load
    let arena = Arc::new(ArenaMempool::new(
        256 * 1024 * 1024, // 256MB capacity
        100_000,           // 100k batches
    ));
    let arena_capacity = 256 * 1024 * 1024; // Update arena_capacity to reflect the new explicit size
    info!(
        "üì¶ Arena mempool initialized ({} zones, {}M capacity, partitioned)",
        pos::ARENA_MAX_WORKERS * 16,
        arena_capacity / 1_000_000
    );

    // Channel for discovered peers
    let (peer_tx, mut peer_rx) = mpsc::channel::<DiscoveredPeer>(100);

    // Start QUIC transport
    let quic_endpoint = start_quic_server(args.port).await?;
    let quic_endpoint_clone = quic_endpoint.clone();

    let total_received = Arc::new(AtomicU64::new(0));
    let total_received_clone = total_received.clone();
    let arena_clone = arena.clone();
    let node_id = generate_node_id(&node_name);

    // Spawn QUIC listener
    tokio::spawn(async move {
        handle_quic_connections(
            quic_endpoint_clone,
            arena_clone,
            total_received_clone,
            node_id,
            args.port,
            num_pairs,
        )
        .await;
    });

    info!("‚úÖ L2 (QUIC) listening on 0.0.0.0:{}", args.port);

    // Start mDNS discovery
    let quic_port = args.port;
    tokio::spawn(async move {
        if let Err(e) = start_mdns_discovery(args.discovery_port, quic_port, peer_tx).await {
            warn!("mDNS discovery error: {}", e);
        }
    });

    info!("‚úÖ L0 (mDNS) discovery service started");

    let batch_size = args.batch_size;
    let total_processed = Arc::new(AtomicU64::new(0));
    let total_processed_clone = total_processed.clone();
    let total_applied = Arc::new(AtomicU64::new(0));

    // Initialize Shard Workers (Receivers)
    // Returns vector of Senders to be shared among Consumers
    let total_rejected = Arc::new(AtomicU64::new(0));
    let shard_senders = start_shard_workers(
        ledger.clone(),
        total_applied.clone(),
        total_rejected.clone(),
    );
    let shard_senders = Arc::new(shard_senders);

    // Spawn Consumers (Consumers pull from their partition)
    for i in 0..num_pairs {
        let arena_consumer = arena.clone();
        let total_clone = total_processed_clone.clone();
        let shard_senders_clone = shard_senders.clone();
        let mut sequencer = Sequencer::new(sequencer_config.clone());

        std::thread::spawn(move || {
            consumer_loop_blocking(
                i, // worker_id
                &mut sequencer,
                arena_consumer,
                total_clone,
                batch_size,
                shard_senders_clone,
            );
        });
    }

    info!("‚úÖ {} Consumer threads started", num_pairs);

    info!("‚úÖ {} Consumer threads started", num_pairs);

    // Spawn Producers
    if args.producer {
        let tps_per_producer = args.tps / num_pairs as u64;

        for i in 0..num_pairs {
            let arena_producer = arena.clone();
            let node_id_producer = node_id;
            let smart_gen = args.smart_gen;

            tokio::spawn(async move {
                producer_loop(
                    arena_producer,
                    tps_per_producer,
                    node_id_producer,
                    smart_gen,
                    i, // worker_id
                )
                .await;
            });
        }

        info!(
            "‚úÖ {} Producer loops started (target: {} TPS total)",
            num_pairs, args.tps
        );
    }

    println!();
    println!("üåê Starting Geometric Turbine Network Test...");
    println!("üöÄ Geometric Turbine Network (HashReveal Enabled)");
    println!();

    // Main event loop
    loop {
        tokio::select! {
            Some(peer_info) = peer_rx.recv() => {
                info!("üîé NEW PEER DISCOVERED via mDNS!");
                if let Some(ip) = extract_ip_from_multiaddr(&peer_info.multiaddr) {
                    let target_port = if args.port == 9000 { 9001 } else { 9000 };
                    let endpoint = quic_endpoint.clone();
                    tokio::spawn(async move {
                        if let Err(e) = connect_to_peer(endpoint, ip, target_port).await {
                            warn!("Failed to connect to peer: {}", e);
                        }
                    });
                }
            }

            _ = tokio::time::sleep(Duration::from_secs(5)) => {
                let sequenced = total_processed.load(Ordering::Relaxed);
                let _applied = total_applied.load(Ordering::Relaxed);
                let stats = arena.stats();
                let applied = total_applied.load(Ordering::Relaxed);
                let rejected = total_rejected.load(Ordering::Relaxed);
                let lag = sequenced.saturating_sub(applied + rejected);

                info!("üìä Heartbeat | Arena: ready={} free={} | Sequenced: {} | Applied: {} | Rejected: {} | Lag: {}",
                      stats.zones_ready, stats.zones_free, sequenced, applied, rejected, lag);
            }
        }
    }
}

// Assuming shard_worker_loop is defined elsewhere, adding the log at its end.
// This assumes the structure of shard_worker_loop ends with a loop and then the function itself.
// The exact placement depends on the full definition of shard_worker_loop.
// For this edit, I'm placing it after the assumed end of the function's main loop.
// If shard_worker_loop is not defined in the provided context, this part is an assumption.
// The instruction implies it should be placed after the closing braces of the loop and function.
// Since the full definition of `shard_worker_loop` is not provided, I'm placing it
// where the instruction suggests it would logically fit after the function's main loop.
// This line would typically be inside the `shard_worker_loop` function, after its main processing loop.
// For the purpose of this diff, I'm placing it as indicated by the instruction's context.

fn start_shard_workers(
    ledger: Arc<GeometricLedger>,
    total_applied: Arc<AtomicU64>,
    total_rejected: Arc<AtomicU64>,
) -> Vec<std::sync::mpsc::SyncSender<ShardWork>> {
    let num_shards = 8; // Match thread count or partition count
    let mut senders = Vec::new();

    for i in 0..num_shards {
        // Reduced channel size to prevent memory pressure
        let (tx, rx) = std::sync::mpsc::sync_channel(1_000);
        senders.push(tx);
        let ledger_clone = ledger.clone();
        let total_clone = total_applied.clone();
        let rejected_clone = total_rejected.clone();

        std::thread::spawn(move || {
            shard_worker_loop(i, ledger_clone, rx, total_clone, rejected_clone);
        });
    }

    info!("üíæ Starting {} shard workers (Direct Dispatch)", num_shards);
    senders
}

/// Start mDNS discovery service
async fn start_mdns_discovery(
    listen_port: u16,
    quic_port: u16,
    peer_tx: mpsc::Sender<DiscoveredPeer>,
) -> Result<(), Box<dyn std::error::Error>> {
    let id_keys = libp2p::identity::Keypair::generate_ed25519();
    let local_peer_id = PeerId::from(id_keys.public());
    let mdns = mdns::tokio::Behaviour::new(mdns::Config::default(), local_peer_id)?;
    let behaviour = DiscoveryBehaviour { mdns };
    let mut swarm = SwarmBuilder::with_new_identity()
        .with_tokio()
        .with_tcp(
            tcp::Config::default(),
            noise::Config::new,
            yamux::Config::default,
        )?
        .with_behaviour(|_key| Ok(behaviour))?
        .build();
    let listen_addr = if listen_port == 0 {
        "/ip4/0.0.0.0/tcp/0".parse()?
    } else {
        format!("/ip4/0.0.0.0/tcp/{}", listen_port).parse()?
    };
    swarm.listen_on(listen_addr)?;
    info!(
        "üì° L0 Discovery service started (advertising QUIC port {})",
        quic_port
    );
    loop {
        if let Some(event) = swarm.next().await {
            if let SwarmEvent::Behaviour(DiscoveryBehaviourEvent::Mdns(mdns::Event::Discovered(
                list,
            ))) = event
            {
                for (peer_id, multiaddr) in list {
                    let _ = peer_tx
                        .send(DiscoveredPeer {
                            peer_id,
                            multiaddr,
                            quic_port,
                        })
                        .await;
                }
            }
        }
    }
}

/// Start QUIC server
async fn start_quic_server(port: u16) -> Result<Endpoint, Box<dyn std::error::Error>> {
    let (cert, key) = generate_self_signed_cert()?;
    let server_config = ServerConfig::with_single_cert(vec![cert], key)?;
    let addr = SocketAddr::new(IpAddr::V4(Ipv4Addr::UNSPECIFIED), port);
    let mut endpoint = Endpoint::server(server_config, addr)?;
    let crypto = rustls::ClientConfig::builder()
        .with_safe_defaults()
        .with_custom_certificate_verifier(Arc::new(NoCertificateVerification))
        .with_no_client_auth();
    let mut client_config = ClientConfig::new(Arc::new(crypto));
    client_config.transport_config(Arc::new(create_transport_config()));
    endpoint.set_default_client_config(client_config);
    Ok(endpoint)
}

/// Handle incoming QUIC connections
async fn handle_quic_connections(
    endpoint: Endpoint,
    arena: Arc<ArenaMempool>, // Changed from Mempool
    total_received: Arc<AtomicU64>,
    _node_id: [u8; 32],
    _listen_port: u16,
    num_active_workers: usize,
) {
    let next_worker = Arc::new(AtomicUsize::new(0));

    while let Some(connecting) = endpoint.accept().await {
        let arena = arena.clone();
        let total_received = total_received.clone();
        let next_worker = next_worker.clone();

        tokio::spawn(async move {
            match connecting.await {
                Ok(connection) => {
                    while let Ok((mut send, mut recv)) = connection.accept_bi().await {
                        let arena = arena.clone();
                        let total_received = total_received.clone();
                        let next_worker = next_worker.clone();
                        tokio::spawn(async move {
                            if let Ok(data) = recv.read_to_end(10 * 1024 * 1024).await {
                                if let Ok(msg) = deserialize_message(&data) {
                                    match msg {
                                        WireMessage::TransactionSubmission { tx } => {
                                            let tx: Transaction = tx.into();
                                            // Use legacy submission for random individual txs
                                            if arena.submit_batch(&[tx]) {
                                                total_received.fetch_add(1, Ordering::Relaxed);
                                            }
                                            let _ = send.finish().await;
                                        }
                                        WireMessage::BatchSubmission { txs } => {
                                            let txs: Vec<Transaction> =
                                                txs.into_iter().map(|tx| tx.into()).collect();
                                            let count = txs.len();

                                            // Round-robin to active workers only
                                            let worker_id = next_worker
                                                .fetch_add(1, Ordering::Relaxed)
                                                % num_active_workers;

                                            // Try to submit to the chosen worker's partition
                                            if arena.submit_batch_partitioned(worker_id, &txs) {
                                                total_received
                                                    .fetch_add(count as u64, Ordering::Relaxed);
                                            } else {
                                                // If full, try one more random worker as fallback
                                                let retry_worker =
                                                    (worker_id + 1) % num_active_workers;
                                                if arena
                                                    .submit_batch_partitioned(retry_worker, &txs)
                                                {
                                                    total_received
                                                        .fetch_add(count as u64, Ordering::Relaxed);
                                                } else {
                                                    warn!(
                                                        "Arena full! Dropped batch of {} txs",
                                                        count
                                                    );
                                                }
                                            }
                                            let _ = send.finish().await;
                                        }
                                        WireMessage::MempoolStatus => {
                                            let stats = arena.stats();
                                            let response = WireMessage::MempoolStatusResponse {
                                                pending_count: stats.total_submitted as u64, // Approximate pending
                                                capacity_tps: 2_000_000, // Hardcoded for now, could be dynamic
                                            };
                                            let msg_bytes = serialize_message(&response).unwrap();
                                            let _ = send.write_all(&msg_bytes).await;
                                            let _ = send.finish().await;
                                        }
                                        _ => {
                                            let _ = send.finish().await;
                                        }
                                    }
                                } else {
                                    // Log failure to help debug Windows/Mac mismatch
                                    if let Err(e) = deserialize_message(&data) {
                                        warn!(
                                            "Failed to deserialize message ({} bytes): {}",
                                            data.len(),
                                            e
                                        );
                                    }
                                }
                            }
                        });
                    }
                }
                Err(_) => {}
            }
        });
    }
}

// ... (Helper functions like connect_to_peer, etc. remain the same) ...

async fn connect_to_peer(
    endpoint: Endpoint,
    ip: IpAddr,
    port: u16,
) -> Result<(), Box<dyn std::error::Error>> {
    let addr = SocketAddr::new(ip, port);
    let connection = endpoint.connect(addr, "localhost")?.await?;
    let (mut send, _) = connection.open_bi().await?;
    let handshake = WireMessage::Handshake {
        version: PROTOCOL_VERSION,
        geometric_id: [0u8; 32],
        listen_port: port,
    };
    let msg_bytes = serialize_message(&handshake)?;
    send.write_all(&msg_bytes).await?;
    send.finish().await?;
    Ok(())
}

fn generate_self_signed_cert(
) -> Result<(rustls::Certificate, rustls::PrivateKey), Box<dyn std::error::Error>> {
    let cert = rcgen::generate_simple_self_signed(vec!["localhost".into()])?;
    Ok((
        rustls::Certificate(cert.serialize_der()?),
        rustls::PrivateKey(cert.serialize_private_key_der()),
    ))
}

fn create_transport_config() -> quinn::TransportConfig {
    let mut config = quinn::TransportConfig::default();
    config.max_concurrent_bidi_streams(100u32.into());
    config
}

fn extract_ip_from_multiaddr(multiaddr: &Multiaddr) -> Option<IpAddr> {
    for proto in multiaddr.iter() {
        match proto {
            libp2p::multiaddr::Protocol::Ip4(ip) => return Some(IpAddr::V4(ip)),
            libp2p::multiaddr::Protocol::Ip6(ip) => return Some(IpAddr::V6(ip)),
            _ => {}
        }
    }
    None
}

fn generate_node_id(name: &str) -> [u8; 32] {
    let hash = blake3::hash(name.as_bytes());
    *hash.as_bytes()
}

struct NoCertificateVerification;
impl rustls::client::ServerCertVerifier for NoCertificateVerification {
    fn verify_server_cert(
        &self,
        _: &rustls::Certificate,
        _: &[rustls::Certificate],
        _: &rustls::ServerName,
        _: &mut dyn Iterator<Item = &[u8]>,
        _: &[u8],
        _: std::time::SystemTime,
    ) -> Result<rustls::client::ServerCertVerified, rustls::Error> {
        Ok(rustls::client::ServerCertVerified::assertion())
    }
}

// ============================================================================
// THE HEARTBEAT: Producer & Consumer Loops
// ============================================================================

async fn producer_loop(
    arena: Arc<ArenaMempool>,
    target_tps: u64,
    _sender_id: [u8; 32],
    smart_gen: bool,
    worker_id: usize,
) {
    // use rand::rngs::StdRng;
    // use rand::{Rng, SeedableRng};
    use std::time::Instant;

    let mut nonce: u64 = (worker_id as u64) * 100_000_000;
    let mut last_report = Instant::now();
    let mut produced_since_report: u64 = 0;

    // Match burst size to Arena Zone Size (45,000)
    let burst_size = pos::ZONE_SIZE;
    let interval_ms = (burst_size as u64 * 1000) / target_tps.max(1);

    let node_hash = blake3::hash(b"default-sequencer-node");
    let node_position = pos::calculate_ring_position(&node_hash);

    // Pre-compute accounts to avoid hashing in the hot loop
    let accounts: std::sync::Arc<Vec<[u8; 32]>> = std::sync::Arc::new(
        (0u64..10000)
            .map(|i| {
                let seed = i.to_le_bytes();
                let hash = blake3::hash(&seed);
                *hash.as_bytes()
            })
            .collect(),
    );

    info!(
        "üè≠ Producer #{} starting: {} TPS target (Partition {})",
        worker_id, target_tps, worker_id
    );

    loop {
        let burst_start = Instant::now();
        let arena_clone = arena.clone();
        let accounts_clone = accounts.clone();
        let current_nonce = nonce;

        let submitted = tokio::task::spawn_blocking(move || {
            let timestamp = std::time::SystemTime::now()
                .duration_since(std::time::UNIX_EPOCH)
                .unwrap()
                .as_millis() as u64;
            let max_range = pos::MAX_DISTANCE;

            // Pre-allocate batch vector
            let mut batch = Vec::with_capacity(burst_size);

            for i in 0..burst_size {
                // Deterministic lookup (O(1))
                let sender_idx = ((current_nonce + i as u64) % 10000) as usize;
                let sender = accounts_clone[sender_idx];

                let recipient_idx = (((current_nonce + i as u64) / 10000) % 10000) as usize;
                let recipient = accounts_clone[recipient_idx];

                let sender_nonce = (current_nonce + i as u64) / 10000;
                let amount = ((i as u64 * 7919) % 9999) + 1;

                let tx = Transaction::new_fast(
                    sender,
                    TransactionPayload::Transfer {
                        recipient,
                        amount,
                        nonce: sender_nonce,
                    },
                    sender_nonce,
                    timestamp + i as u64,
                    [0u8; 32], // HashReveal secret
                );

                if smart_gen {
                    let tx_hash = tx.hash();
                    let tx_position = pos::calculate_ring_position(&tx_hash);
                    let distance = if tx_position >= node_position {
                        tx_position - node_position
                    } else {
                        (u64::MAX - node_position) + tx_position + 1
                    };
                    if distance <= max_range {
                        batch.push(tx);
                    }
                } else {
                    batch.push(tx);
                }
            }

            // Submit entire batch to partition
            if !batch.is_empty() {
                if arena_clone.submit_batch_partitioned(worker_id, &batch) {
                    return batch.len() as u64;
                }
            }
            0
        })
        .await
        .unwrap_or(0);

        nonce += burst_size as u64;
        produced_since_report += submitted;

        if worker_id == 0 && last_report.elapsed() >= Duration::from_secs(5) {
            let actual_tps = produced_since_report as f64 / last_report.elapsed().as_secs_f64();
            info!(
                "üè≠ Producer #{} stats: {} tx/s",
                worker_id, actual_tps as u64
            );
            produced_since_report = 0;
            last_report = Instant::now();
        }

        let elapsed = burst_start.elapsed().as_millis() as u64;
        if elapsed < interval_ms {
            tokio::time::sleep(Duration::from_millis(interval_ms - elapsed)).await;
        } else {
            tokio::task::yield_now().await;
        }
    }
}

fn consumer_loop_blocking(
    worker_id: usize,
    sequencer: &mut Sequencer,
    arena: Arc<ArenaMempool>,
    total_processed: Arc<AtomicU64>,
    _batch_size: usize,
    shard_senders: Arc<Vec<std::sync::mpsc::SyncSender<ShardWork>>>,
) {
    use std::time::Instant;
    let mut last_report = Instant::now();
    let mut batches_since_report: u64 = 0;
    let mut processed_since_report: u64 = 0;

    tracing::info!(
        "üîÑ Consumer #{} starting (Partition {})",
        worker_id,
        worker_id
    );

    loop {
        // Pull from partition
        if let Some(txs) = arena.pull_batch_partitioned(worker_id) {
            if !txs.is_empty() {
                let _batch_start = Instant::now();
                let (accepted, rejected) = sequencer.process_batch(txs);

                if let Some(batch) = sequencer.finalize_batch() {
                    let batch = Arc::new(batch);
                    let total_txs = batch.transactions.len();
                    let num_shards = shard_senders.len();
                    let chunk_size = (total_txs + num_shards - 1) / num_shards;

                    for (i, sender) in shard_senders.iter().enumerate() {
                        let start = i * chunk_size;
                        if start >= total_txs {
                            break;
                        }
                        let count = std::cmp::min(chunk_size, total_txs - start);
                        let work = ShardWork {
                            batch: batch.clone(),
                            start,
                            count,
                        };
                        let _ = sender.send(work);
                    }
                    batches_since_report += 1;
                }

                // Update metrics for ALL processed transactions (accepted or rejected)
                total_processed.fetch_add((accepted + rejected) as u64, Ordering::Relaxed);
                processed_since_report += (accepted + rejected) as u64;
            }
        } else {
            // No ready zones in partition, yield
            std::thread::sleep(Duration::from_micros(50));
        }

        if worker_id == 0 && last_report.elapsed() >= Duration::from_secs(5) {
            let actual_tps = processed_since_report as f64 / last_report.elapsed().as_secs_f64();
            let stats = arena.stats();
            tracing::info!(
                "‚ö° Consumer Stats: {} batches, {} tx/s (Sequenced) | Arena: {} ready",
                batches_since_report,
                actual_tps as u64,
                stats.zones_ready
            );
            batches_since_report = 0;
            processed_since_report = 0;
            last_report = Instant::now();
        }
    }
}

// Ledger Worker Loop removed (Decoupled architecture)

struct ShardWork {
    batch: Arc<pos::Batch>,
    start: usize,
    count: usize,
}

fn shard_worker_loop(
    shard_id: usize,
    ledger: Arc<GeometricLedger>,
    work_rx: std::sync::mpsc::Receiver<ShardWork>,
    total: Arc<AtomicU64>,
    total_rejected: Arc<AtomicU64>,
) {
    use pos::Account;
    use std::collections::HashMap;

    // Process work items as they arrive
    while let Ok(work) = work_rx.recv() {
        let mut cache = HashMap::new();
        let mut applied_count = 0;
        let mut rejected_count = 0;

        // Zero-Copy: Access the slice of the shared batch
        let txs = &work.batch.transactions[work.start..work.start + work.count];

        // DEBUG LOGGING
        if shard_id == 0 {
            // tracing::info!("Worker #{} received batch chunk of {} txs", shard_id, txs.len());
        }

        for ptx in txs {
            if let pos::TransactionPayload::Transfer {
                recipient, amount, ..
            } = ptx.tx.payload
            {
                let sender = ptx.tx.sender;
                // Simple ledger application logic
                let mut s_acc = cache.remove(&sender).unwrap_or_else(|| {
                    ledger.get(&sender).unwrap_or(Account {
                        pubkey: sender,
                        ..Default::default()
                    })
                });

                if s_acc.balance >= amount {
                    s_acc.balance -= amount;
                    cache.insert(sender, s_acc);
                    let mut r_acc = cache.remove(&recipient).unwrap_or_else(|| {
                        ledger.get(&recipient).unwrap_or(Account {
                            pubkey: recipient,
                            ..Default::default()
                        })
                    });
                    r_acc.balance += amount;
                    cache.insert(recipient, r_acc);
                    applied_count += 1;
                } else {
                    // tracing::warn!(
                    //     "Insufficient balance for sender {:?}: {} < {}",
                    //     &sender[0..4],
                    //     s_acc.balance,
                    //     amount
                    // );
                    cache.insert(sender, s_acc); // Return to cache
                    rejected_count += 1;
                }
            }
        }
        if !cache.is_empty() {
            let updates: Vec<_> = cache.into_iter().collect();
            let _ = ledger.update_batch(&updates);
            total.fetch_add(applied_count, Ordering::Relaxed);
            total_rejected.fetch_add(rejected_count, Ordering::Relaxed);
        }
    }
}
