//! Geometric Ledger - Persistent, Crash-Safe, Windows-Compatible
//! Uses Write-Ahead Log (WAL) for 30M TPS Sequential Persistence

use memmap2::{MmapMut, MmapOptions};
use std::fs::{self, File, OpenOptions};
use std::io::{Write, BufWriter, Read, BufReader};
use std::path::Path;
use std::sync::atomic::{AtomicU64, AtomicU32, Ordering};
use std::sync::Mutex;
use rayon::prelude::*;

// Fixed-size account (pad to 128 bytes)
// Layout: pubkey(32) + balance(8) + nonce(8) + last_modified(8) + auth_root(32) + padding(40) = 128
#[repr(C, align(128))]
#[derive(Copy, Clone, Debug)]
pub struct Account {
    /// Public key (Ed25519)
    pub pubkey: [u8; 32],
    /// Token balance
    pub balance: u64,
    /// Transaction nonce (increments with each tx)
    pub nonce: u64,
    /// Last modification timestamp
    pub last_modified: u64,
    /// Current tip of the hash chain (for fast-path auth)
    /// This is updated after each HashReveal verification:
    /// auth_root = revealed_secret
    pub auth_root: [u8; 32],
    /// Padding to maintain 128-byte alignment
    pub _padding: [u8; 40],
}

impl Default for Account {
    fn default() -> Self {
        Self {
            pubkey: [0u8; 32],
            balance: 0,
            nonce: 0,
            last_modified: 0,
            auth_root: [0u8; 32],
            _padding: [0u8; 40],
        }
    }
}

/// Transaction verification result
#[derive(Debug, Clone, Copy, PartialEq, Eq)]
pub enum VerifyResult {
    /// Transaction is valid
    Valid,
    /// Invalid nonce (replay or out of sequence)
    InvalidNonce,
    /// Hash chain verification failed
    InvalidHashReveal,
    /// Ed25519 signature verification failed  
    InvalidSignature,
    /// Insufficient balance for transfer
    InsufficientBalance,
    /// Account not found
    AccountNotFound,
}

impl Account {
    /// Verify a transaction against this account (FAST PATH: ~15ns for HashReveal)
    /// 
    /// Verification order:
    /// 1. Nonce check (replay protection) - O(1)
    /// 2. Signature verification:
    ///    - HashReveal: blake3(reveal) == auth_root - O(1), ~15ns
    ///    - Ed25519: full signature verification - O(1), ~50,000ns
    #[inline]
    pub fn verify_transaction(&self, tx: &crate::types::Transaction) -> VerifyResult {
        use crate::types::SignatureType;
        
        // 1. Nonce check (replay protection)
        // Transaction nonce must be exactly account.nonce + 1
        if tx.nonce != self.nonce + 1 {
            return VerifyResult::InvalidNonce;
        }
        
        // 2. Signature verification based on type
        match &tx.signature {
            SignatureType::HashReveal(revealed_secret) => {
                // FAST PATH: ~15ns
                // Verify: blake3(revealed_secret) == stored auth_root
                let calculated_root = blake3::hash(revealed_secret);
                if calculated_root.as_bytes() != &self.auth_root {
                    return VerifyResult::InvalidHashReveal;
                }
            }
            SignatureType::Ed25519(signature) => {
                // SLOW PATH: ~50,000ns
                // Full Ed25519 verification against pubkey
                use ed25519_dalek::{Signature, VerifyingKey, Verifier};
                
                let Ok(verifying_key) = VerifyingKey::from_bytes(&self.pubkey) else {
                    return VerifyResult::InvalidSignature;
                };
                // Signature::from_slice returns Result, from_bytes takes &[u8; 64] directly
                let sig = Signature::from_bytes(signature);
                
                let message = tx.signing_message();
                if verifying_key.verify(&message, &sig).is_err() {
                    return VerifyResult::InvalidSignature;
                }
            }
        }
        
        VerifyResult::Valid
    }
    
    /// Apply a verified transaction to this account
    /// Returns the new account state
    /// 
    /// IMPORTANT: Call verify_transaction() first!
    pub fn apply_transaction(&self, tx: &crate::types::Transaction, timestamp: u64) -> Result<Account, VerifyResult> {
        use crate::types::{SignatureType, TransactionPayload};
        
        let mut new_account = *self;
        new_account.nonce = tx.nonce;
        new_account.last_modified = timestamp;
        
        // Update auth_root based on signature type
        match &tx.signature {
            SignatureType::HashReveal(revealed_secret) => {
                // Rotate auth_root to the revealed secret
                // Next tx must reveal preimage of this secret
                new_account.auth_root = *revealed_secret;
            }
            SignatureType::Ed25519(_) => {
                // Ed25519 transactions can optionally set a new auth_root
                // This is the "refueling" operation
                // For now, we don't change auth_root on Ed25519 txs
                // A separate SetAuthRoot payload type could be added
            }
        }
        
        // Apply payload effects
        match &tx.payload {
            TransactionPayload::Transfer { amount, .. } => {
                if new_account.balance < *amount {
                    return Err(VerifyResult::InsufficientBalance);
                }
                new_account.balance -= *amount;
            }
            TransactionPayload::SetAuthRoot { new_auth_root } => {
                // Refueling: set new auth_root for future fast-path transactions
                // This must be an Ed25519 signed transaction
                new_account.auth_root = *new_auth_root;
            }
        }
        
        Ok(new_account)
    }
    
    /// Initialize auth_root from a seed (for account creation)
    /// Computes H^n(seed) where n = chain_length
    /// Returns the commitment (H^0) that should be stored as auth_root
    pub fn initialize_auth_chain(seed: &[u8; 32], chain_length: u32) -> [u8; 32] {
        let mut current = *seed;
        for _ in 0..chain_length {
            current = *blake3::hash(&current).as_bytes();
        }
        current
    }
}

const ACCOUNT_SIZE: usize = 128;
const SHARD_COUNT: usize = 256;
const ACCOUNTS_PER_SHARD: usize = 10_000;

// Atomic index slot
// OPTIMIZATION: align(64) for cache line alignment - prevents false sharing between adjacent slots
// Each slot gets its own cache line, eliminating CPU coherency traffic during concurrent CAS operations
// Trade-off: Uses more memory (~32GB for 256M slots) but massively reduces contention at high TPS
#[repr(C, align(64))]
struct AtomicSlot {
    data: AtomicU64,
    _padding: [u8; 56], // Pad to 64 bytes (8 + 56 = 64)
}

impl AtomicSlot {
    const EMPTY: u64 = 0;

    fn new() -> Self {
        Self {
            data: AtomicU64::new(Self::EMPTY),
            _padding: [0u8; 56],
        }
    }

    fn pack(hash: u32, offset: u32, distance: u8) -> u64 {
        let occupied = 1u64 << 63;
        let dist = (distance as u64) << 56;
        let off = (offset as u64) << 32;
        occupied | dist | off | (hash as u64)
    }

    fn unpack(&self) -> Option<(u32, u32, u8)> {
        let val = self.data.load(Ordering::Acquire);
        if val == Self::EMPTY { return None; }
        let hash = (val & 0xFFFF_FFFF) as u32;
        let offset = ((val >> 32) & 0x00FF_FFFF) as u32;
        let distance = ((val >> 56) & 0x7F) as u8;
        Some((hash, offset, distance))
    }

}

const INDEX_SIZE: usize = ACCOUNTS_PER_SHARD * 2;

struct Shard {
    mmap: MmapMut,
    index: Box<[AtomicSlot]>,
    wal: Mutex<BufWriter<File>>,
    wal_path: std::path::PathBuf,
    account_count: AtomicU32,
    write_count: AtomicU64,
}

impl Shard {
    fn open(data_dir: &Path, shard_id: usize) -> Result<Self, String> {
        let path = data_dir.join(format!("shard_{:03}.bin", shard_id));
        let wal_path = data_dir.join(format!("shard_{:03}.wal", shard_id));
        let size = (ACCOUNTS_PER_SHARD * ACCOUNT_SIZE) as u64;

        if let Some(parent) = path.parent() {
            fs::create_dir_all(parent).map_err(|e| e.to_string())?;
        }

        let file = OpenOptions::new()
            .read(true)
            .write(true)
            .create(true)
            .open(&path)
            .map_err(|e| format!("Failed to open shard file: {}", e))?;

        if file.metadata().map_err(|e| e.to_string())?.len() < size {
            file.set_len(size).map_err(|e| format!("Failed to set length: {}", e))?;
        }

        let mmap = unsafe {
            MmapOptions::new()
                .len(size as usize)
                .map_mut(&file)
                .map_err(|e| format!("Failed to mmap: {}", e))?
        };

        // Open WAL file for sequential writes
        let wal_file = OpenOptions::new()
            .read(true)
            .write(true)
            .create(true)
            .append(true)
            .open(&wal_path)
            .map_err(|e| format!("Failed to open WAL: {}", e))?;

        let mut index_vec = Vec::with_capacity(INDEX_SIZE);
        for _ in 0..INDEX_SIZE {
            index_vec.push(AtomicSlot::new());
        }
        let index = index_vec.into_boxed_slice();

        let mut shard = Self {
            mmap,
            index,
            wal: Mutex::new(BufWriter::with_capacity(1024 * 1024, wal_file)), // 1MB buffer
            wal_path,
            account_count: AtomicU32::new(0),
            write_count: AtomicU64::new(0),
        };

        // Replay WAL before rebuilding index
        shard.replay_wal()?;
        shard.rebuild_index()?;
        Ok(shard)
    }

    /// Replay WAL entries to recover from crash
    fn replay_wal(&mut self) -> Result<(), String> {
        let file = OpenOptions::new()
            .read(true)
            .open(&self.wal_path)
            .map_err(|e| format!("Failed to open WAL for replay: {}", e))?;
        
        let file_len = file.metadata().map_err(|e| e.to_string())?.len();
        if file_len == 0 {
            return Ok(());
        }

        let mut reader = BufReader::new(file);
        let mut buf = [0u8; ACCOUNT_SIZE];
        let mut replayed = 0u64;

        while reader.read_exact(&mut buf).is_ok() {
            let account: Account = unsafe { std::ptr::read_unaligned(buf.as_ptr() as *const Account) };
            if account.pubkey != [0u8; 32] {
                // Write directly to mmap (index rebuilt after)
                // For replay, we just write to next available offset
                let offset = self.account_count.fetch_add(1, Ordering::Relaxed) as usize;
                if offset < ACCOUNTS_PER_SHARD {
                    self.write_account(offset, &account);
                    replayed += 1;
                }
            }
        }

        if replayed > 0 {
            println!("  Shard replayed {} WAL entries", replayed);
        }

        Ok(())
    }

    fn rebuild_index(&mut self) -> Result<(), String> {
        let mut count = 0u32;
        for offset in 0..ACCOUNTS_PER_SHARD {
            let account = self.read_account(offset);
            if account.pubkey != [0u8; 32] {
                let hash = hash_pubkey(&account.pubkey);
                let mut idx = (hash as usize) % INDEX_SIZE;
                let mut distance = 0u8;
                loop {
                    let slot = &self.index[idx];
                    if slot.data.load(Ordering::Relaxed) == AtomicSlot::EMPTY {
                        slot.data.store(
                            AtomicSlot::pack(hash, offset as u32, distance),
                            Ordering::Relaxed
                        );
                        count += 1;
                        break;
                    }
                    idx = (idx + 1) % INDEX_SIZE;
                    distance += 1;
                    if distance > 127 { break; }
                }
            }
        }
        self.account_count.store(count, Ordering::Relaxed);
        Ok(())
    }

    fn get(&self, pubkey: &[u8; 32]) -> Option<Account> {
        let hash = hash_pubkey(pubkey);
        let mut idx = (hash as usize) % INDEX_SIZE;
        for _ in 0..128 {
            if let Some((slot_hash, offset, _)) = self.index[idx].unpack() {
                if slot_hash == hash {
                    let acc = self.read_account(offset as usize);
                    if &acc.pubkey == pubkey {
                        return Some(acc);
                    }
                }
            } else {
                return None;
            }
            idx = (idx + 1) % INDEX_SIZE;
        }
        None
    }

    fn write_account(&self, offset: usize, account: &Account) {
        let start = offset * ACCOUNT_SIZE;
        let dst = unsafe { self.mmap.as_ptr().add(start) as *mut u8 };
        unsafe {
            std::ptr::copy_nonoverlapping(
                account as *const Account as *const u8,
                dst,
                ACCOUNT_SIZE
            );
        }
    }

    fn read_account(&self, offset: usize) -> Account {
        let start = offset * ACCOUNT_SIZE;
        let src = unsafe { self.mmap.as_ptr().add(start) };
        unsafe {
            std::ptr::read_unaligned(src as *const Account)
        }
    }

    fn flush(&self) -> Result<(), String> {
        // Flush mmap to disk
        self.mmap.flush().map_err(|e| e.to_string())?;
        
        // Flush and truncate WAL (data is now safely in mmap)
        {
            let mut wal = self.wal.lock().map_err(|_| "WAL lock poisoned")?;
            wal.flush().map_err(|e| e.to_string())?;
        }
        
        // Truncate WAL file to zero
        let wal_file = OpenOptions::new()
            .write(true)
            .truncate(true)
            .open(&self.wal_path)
            .map_err(|e| format!("Failed to truncate WAL: {}", e))?;
        drop(wal_file);
        
        Ok(())
    }

    /// Batch insert with WAL for durability
    /// Phase 0: Sequential write to WAL (durability)
    /// Phase 1: Classify as updates vs inserts (parallel reads)
    /// Phase 2: Allocate offsets for inserts (single atomic)
    /// Phase 3: Write all accounts to mmap (parallel writes)
    /// Phase 4: Update index entries (parallel CAS)
    fn insert_batch(&self, updates: &[(&[u8; 32], &Account)]) -> Result<(), String> {
        if updates.is_empty() {
            return Ok(());
        }

        // Phase 0: Sequential WAL write (durability - single syscall for whole batch)
        {
            let mut wal = self.wal.lock().map_err(|_| "WAL lock poisoned")?;
            for (_, acc) in updates {
                let bytes = unsafe {
                    std::slice::from_raw_parts(
                        *acc as *const Account as *const u8,
                        ACCOUNT_SIZE
                    )
                };
                wal.write_all(bytes).map_err(|e| e.to_string())?;
            }
            // Don't flush here - let OS buffer for speed
            // Data is durable once in page cache
        }

        // Phase 1: Parallel classification - find existing accounts
        let classifications: Vec<_> = updates.par_iter().map(|(pk, _)| {
            let hash = hash_pubkey(pk);
            let mut idx = (hash as usize) % INDEX_SIZE;

            // Search for existing entry
            for _ in 0..128 {
                if let Some((slot_hash, offset, _)) = self.index[idx].unpack() {
                    if slot_hash == hash {
                        let acc = self.read_account(offset as usize);
                        if &acc.pubkey == *pk {
                            return (hash, idx, Some(offset)); // Update existing
                        }
                    }
                } else {
                    return (hash, idx, None); // Insert new at this slot
                }
                idx = (idx + 1) % INDEX_SIZE;
            }
            (hash, idx, None) // Insert new (fallback)
        }).collect();

        // Phase 2: Allocate offsets for new accounts (single atomic batch)
        let insert_count = classifications.iter().filter(|(_, _, off)| off.is_none()).count();
        let base_offset = if insert_count > 0 {
            let offset = self.account_count.fetch_add(insert_count as u32, Ordering::Relaxed);
            if offset + (insert_count as u32) > ACCOUNTS_PER_SHARD as u32 {
                return Err("Shard full".to_string());
            }
            offset
        } else {
            0
        };

        // Phase 3: Compute offsets for each update (sequential to assign indices)
        let mut insert_offset_counter = base_offset;
        let offsets: Vec<u32> = classifications.iter().map(|(_, _, existing_offset)| {
            if let Some(off) = existing_offset {
                *off
            } else {
                let off = insert_offset_counter;
                insert_offset_counter += 1;
                off
            }
        }).collect();

        // Phase 4: Parallel mmap writes
        updates.par_iter().zip(offsets.par_iter()).for_each(|((_, acc), offset)| {
            self.write_account(*offset as usize, acc);
        });

        // Phase 5: Parallel index updates with CAS (only for new entries)
        classifications.par_iter().zip(offsets.par_iter()).for_each(|((hash, start_idx, existing_offset), offset)| {
            if existing_offset.is_some() {
                return; // Already in index, mmap write is enough
            }

            // New entry - find slot and CAS it in
            let mut idx = *start_idx;
            let mut distance = 0u8;

            loop {
                let slot = &self.index[idx];
                let packed = AtomicSlot::pack(*hash, *offset, distance);

                if slot.data.compare_exchange(
                    AtomicSlot::EMPTY,
                    packed,
                    Ordering::Release,
                    Ordering::Acquire
                ).is_ok() {
                    break;
                }

                idx = (idx + 1) % INDEX_SIZE;
                distance += 1;
                if distance > 127 { break; }
            }
        });

        self.write_count.fetch_add(updates.len() as u64, Ordering::Relaxed);
        Ok(())
    }
}

pub struct GeometricLedger {
    shards: Vec<Shard>,
}

impl GeometricLedger {
    pub fn new(data_dir: impl AsRef<Path>) -> Result<Self, String> {
        let data_dir = data_dir.as_ref();
        let shards: Result<Vec<_>, _> = (0..SHARD_COUNT).into_par_iter().map(|i| {
            Shard::open(data_dir, i)
        }).collect();
        
        let shards = shards?;
        let total: u32 = shards.iter().map(|s| s.account_count.load(Ordering::Relaxed)).sum();
        println!("âœ… Geometric Ledger loaded: {} accounts active.", total);

        Ok(Self { shards })
    }

    pub fn update_batch(&self, updates: &[([u8; 32], Account)]) -> Result<(), String> {
        let mut buckets: Vec<Vec<_>> = (0..SHARD_COUNT).map(|_| Vec::new()).collect();
        for (pk, acc) in updates {
            buckets[pk[0] as usize].push((pk, acc));
        }

        // Use batch insert for each shard (parallel across shards)
        buckets.par_iter().enumerate().try_for_each(|(i, items)| {
            if !items.is_empty() {
                self.shards[i].insert_batch(items)?;
            }
            Ok::<_, String>(())
        })
    }

    pub fn get(&self, pubkey: &[u8; 32]) -> Option<Account> {
        self.shards[pubkey[0] as usize].get(pubkey)
    }
    
    pub fn mint(&self, pubkey: [u8; 32], amount: u64) {
        let mut acc = self.get(&pubkey).unwrap_or_else(|| Account {
            pubkey, ..Default::default()
        });
        acc.balance = acc.balance.saturating_add(amount);
        let _ = self.update_batch(&[(pubkey, acc)]);
    }

    pub fn snapshot(&self) -> Result<(), String> {
        self.shards.par_iter().try_for_each(|s| s.flush())
    }
    
    pub fn stats(&self) -> LedgerStats {
        let total_accounts = self.shards.iter().map(|s| s.account_count.load(Ordering::Relaxed)).sum();
        let total_writes = self.shards.iter().map(|s| s.write_count.load(Ordering::Relaxed)).sum();
        
        LedgerStats {
            total_accounts,
            total_writes,
            shard_count: SHARD_COUNT,
            capacity: (SHARD_COUNT * ACCOUNTS_PER_SHARD) as u64,
        }
    }
}

#[derive(Debug, Clone)]
pub struct LedgerStats {
    pub total_accounts: u32,
    pub total_writes: u64,
    pub shard_count: usize,
    pub capacity: u64,
}

fn hash_pubkey(pubkey: &[u8; 32]) -> u32 {
    let h = blake3::hash(pubkey);
    u32::from_le_bytes(h.as_bytes()[..4].try_into().unwrap())
}